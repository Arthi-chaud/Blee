x-transcoder: &transcoder-base
  image: ghcr.io/zoriya/kyoo_transcoder:4.5.0
  networks:
    default:
      aliases:
        - transcoder
  restart: unless-stopped
  environment:
    - GOCODER_PREFIX=/videos
  volumes:
    - ${DATA_DIR}:/videos:ro
    - ${CACHE_ROOT}:/cache
    - metadata:/metadata

# This compose allows to build the whole project in a "production"-like environment 
services:
  front:
    build:
      context: ./front
    restart: on-failure
    depends_on:
      api:
        condition: service_healthy
    environment:
      - PORT=3000
    ports:
      - "3000:3000"
  api:
    build:
      context: ./api
    ports:
      - "8000:8000"
    restart: on-failure
    depends_on:
      db:
        condition: service_healthy
      mq:
        condition: service_healthy
    volumes:
      - data:${CONFIG_DIR}
    environment:
      - POSTGRES_HOST=db
      - POSTGRES_PORT=5432
      - RABBIT_HOST=mq
      - RABBIT_PORT=5672
    env_file:
      - .env
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- localhost:8000"]
      interval: 5s
      retries: 3
  mq:
    image: rabbitmq:3.13-alpine
    environment:
      - RABBITMQ_DEFAULT_USER=${RABBIT_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBIT_PASS}
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 10s
      timeout: 3s
      retries: 10
  scanner:
    build:
      context: ./scanner
    expose:
      - "8133"
    depends_on:
      api:
        condition: service_healthy
    volumes:
      - ${DATA_DIR}:/videos
      - ./scanner.example.json:/app/scanner.json
    environment:
      - API_URL=http://api:8000
      - WATCH_DIR=/videos
      - SCANNER_API_KEY=${SCANNER_API_KEY}
      - CONFIG_DIR=/app
  matcher:
    build:
      context: ./matcher
    restart: on-failure
    depends_on:
      api:
        condition: service_healthy
    environment:
      - RABBIT_HOST=mq
      - RABBIT_PORT=5672
      - API_URL=http://api:8000
      - MATCHER_API_KEY=${MATCHER_API_KEY}
    env_file:
      - .env
  db:
    image: postgres:alpine3.16
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 3s
      timeout: 5s
      retries: 5
    env_file:
      - .env
    expose:
      - 5432
    volumes:
      - db:/var/lib/postgresql/data
  nginx:
    restart: on-failure
    image: nginx:1.24.0-alpine
    depends_on:
      api:
        condition: service_started
      front:
        condition: service_started
    ports:
      - ${PUBLIC_PORT}:5000
    environment:
      - PORT=5000
      - FRONT_URL=http://front:3000
      - SERVER_URL=http://api:8000
      - TRANSCODER_URL=http://transcoder:7666
      - SCANNER_URL=http://scanner:8133
    volumes:
      - ./nginx.conf.template:/etc/nginx/templates/blee.conf.template:ro
  transcoder:
    <<: *transcoder-base
    profiles: ['', 'cpu']

  transcoder-nvidia:
    <<: *transcoder-base
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - GOCODER_PREFIX=/video
      - GOCODER_HWACCEL=nvidia
    profiles: ['nvidia']

  transcoder-vaapi:
    <<: *transcoder-base
    devices:
      - /dev/dri:/dev/dri
    environment:
      - GOCODER_PREFIX=/video
      - GOCODER_HWACCEL=vaapi
      - GOCODER_VAAPI_RENDERER=${GOCODER_VAAPI_RENDERER:-/dev/dri/renderD128}
    profiles: ['vaapi']
  # qsv is the same setup as vaapi but with the hwaccel env var different
  transcoder-qsv:
    <<: *transcoder-base
    devices:
      - /dev/dri:/dev/dri
    environment:
      - GOCODER_PREFIX=/video
      - GOCODER_HWACCEL=qsv
      - GOCODER_VAAPI_RENDERER=${GOCODER_VAAPI_RENDERER:-/dev/dri/renderD128}
    profiles: ['qsv']
volumes:
  db:
  data:
  metadata:
